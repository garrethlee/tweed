{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/garrethlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/garrethlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/garrethlee/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/garrethlee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/garrethlee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/garrethlee/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=['sentiment', 'id', 'date', 'flag', 'user', 'tweet']\n",
    "data = pd.read_csv('data/tweets.csv', encoding='latin-1', names=columns)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning\n",
    "\n",
    "We'll only need the actual tweet, other columns such as usernames, flags, dates, and id are irrelevant at this level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                              tweet\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop unecessary columns\n",
    "data = data.drop(columns=['id','date','flag','user'])\n",
    "\n",
    "# Original dataset has a scale of (4 - positive, 0 - negative), we will replace this for clarity\n",
    "data['sentiment'] = data['sentiment'].replace({4:1}) \n",
    "\n",
    "# An overview\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can clean up the tweets using regular expressions to remove formatting errors and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unecessary(tweet):\n",
    "    \"\"\"Removes whitespace and non-essential characters from tokens\"\"\"\n",
    "    tweet = re.sub(r\"((www.)?https?:\\/\\/)?[^\\s]*\\.([\\w]{2,3})(\\/\\w*)*\", \"\", tweet) #removes links\n",
    "    tweet = re.sub(r\"(RT )?@\\w+:?\", \"\", tweet) #removes RT and @\n",
    "    tweet = re.sub(r\"[^(a-zA-Z|\\')]\", \" \", tweet)\n",
    "    tweet = ' '.join(tweet.split()) #removes whitespace from text\n",
    "    return tweet\n",
    "\n",
    "data['tweet'] = data['tweet'].apply(remove_unecessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Awww that's a bummer You shoulda got David Car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I dived many times for the ball Managed to sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no it's not behaving at all i'm mad why am i h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>Just woke up Having no school is the best feel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>Very cool to hear old Walt interviews bmta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you ready for your MoJo Makeover Ask me fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>Happy th Birthday to my boo of alll time Tupac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>happy charitytuesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                              tweet\n",
       "0                0  Awww that's a bummer You shoulda got David Car...\n",
       "1                0  is upset that he can't update his Facebook by ...\n",
       "2                0  I dived many times for the ball Managed to sav...\n",
       "3                0     my whole body feels itchy and like its on fire\n",
       "4                0  no it's not behaving at all i'm mad why am i h...\n",
       "...            ...                                                ...\n",
       "1599995          1  Just woke up Having no school is the best feel...\n",
       "1599996          1         Very cool to hear old Walt interviews bmta\n",
       "1599997          1  Are you ready for your MoJo Makeover Ask me fo...\n",
       "1599998          1  Happy th Birthday to my boo of alll time Tupac...\n",
       "1599999          1                               happy charitytuesday\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the data\n",
    "\n",
    "Tokenizing is splitting strings to smaller parts called `tokens`, which will help when categorizing certain words to categories (nouns, adjectives, same words with different tenses, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Split dataset into positive tweets and negative tweets\n",
    "positive_tweets = list(data[data['sentiment'] == 1]['tweet'])\n",
    "negative_tweets = list(data[data['sentiment'] == 0]['tweet'])\n",
    "\n",
    "# Tokenize each tweet (aka split each tweet into smaller subsets)\n",
    "positive_tokens = list(map(lambda tweet: word_tokenize(tweet), positive_tweets[:300000]))\n",
    "negative_tokens = list(map(lambda tweet: word_tokenize(tweet), negative_tweets[:300000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "Per [Wikipedia](en.wikipedia.org/wiki/Text_normalization), the act of reducing a word to its simplest form. Words like catch, catching and caught are reduced to its bare bones, in this case 'catch'.\n",
    "\n",
    "**Stemming**, on the other hand, is removing affixes from words.\n",
    "\n",
    "**Lemmatization** is grouping several words to be analyzed under one group.\n",
    "\n",
    "These are all popular techniques in NLP, but picking between one or the other is down to your preference for speed or accuracy.\n",
    "\n",
    "___\n",
    "\n",
    "For this dataset, we will first get each tweet's word tags using nltk's `pos_tag` function, then group words with similar word tags (nouns, verbs, and adjectives) as one group (*lemmatization*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_tweet(t:list) -> list:\n",
    "    \"\"\"Strips down words in a tweet into its simplest grammatical form\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(t):\n",
    "        if tag[:2] in (\"NN\", \"PRP\"):\n",
    "            pos = \"n\"\n",
    "        elif tag[0] == \"V\":\n",
    "            pos = \"v\"\n",
    "        else:\n",
    "            pos = \"a\"\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word.lower(), pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "lemmatized_positive_tokens = list(map(lemmatize_tweet, positive_tokens))\n",
    "lemmatized_negative_tokens = list(map(lemmatize_tweet, negative_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Noise\n",
    "\n",
    "We remove 'stopwords' to further simplify the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word for word in stopwords.words('english') if word != \"not\"]\n",
    "\n",
    "def clean_token(token:list) -> list:\n",
    "    \"\"\"Removes stopwords from the given token\"\"\"\n",
    "    return [word.lower() for word in token if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_positive_tokens = list(map(clean_token,lemmatized_positive_tokens))\n",
    "cleaned_negative_tokens = list(map(clean_token,lemmatized_negative_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Workflow\n",
    "\n",
    "We have a tweet in text form, we first:\n",
    "\n",
    "1. **Tokenize** the tweet (split into subparts)\n",
    "2. **Lemmatize** the tweet (boil down to simplest word)\n",
    "3. **Clean** the tweet (remove stopwords and other noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield {token:True for token in tweet_tokens}\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(cleaned_positive_tokens)\n",
    "negative_tokens_for_model = get_tweets_for_model(cleaned_negative_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Format the dataset to have its according label\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "# Shuffle the dataset to maintain model objectivity\n",
    "random.shuffle(dataset)\n",
    "\n",
    "# Set 90:10 as training:testing ratio\n",
    "train_data = dataset[:550000]\n",
    "test_data = dataset[550000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.75726\n",
      "Most Informative Features\n",
      "                  asthma = True           Negati : Positi =     32.3 : 1.0\n",
      "               depressed = True           Negati : Positi =     31.8 : 1.0\n",
      "                coughing = True           Negati : Positi =     31.7 : 1.0\n",
      "                  boohoo = True           Negati : Positi =     28.2 : 1.0\n",
      "                  bummed = True           Negati : Positi =     28.1 : 1.0\n",
      "                 electro = True           Positi : Negati =     27.0 : 1.0\n",
      "               heartburn = True           Negati : Positi =     27.0 : 1.0\n",
      "                  unwell = True           Negati : Positi =     27.0 : 1.0\n",
      "                hayfever = True           Negati : Positi =     26.5 : 1.0\n",
      "                  booooo = True           Negati : Positi =     26.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"naivebayes_model.h5\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_pipeline(tweet):\n",
    "    token = word_tokenize(tweet)\n",
    "    lemmatized_token = lemmatize_tweet(token)\n",
    "    cleaned_token = clean_token(lemmatized_token)\n",
    "    return dict((t.lower(),True) for t in cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "def create_url(user):\n",
    "    \"\"\"Creates request URL for the specified username\"\"\"\n",
    "    username = user\n",
    "    url = f\"https://api.twitter.com/2/tweets/search/recent?query=from:{username}&max_results=100\"\n",
    "    return url\n",
    "\n",
    "def get_token():\n",
    "    with open(\"config.yaml\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        return data['twitter_api']['bearer_token']\n",
    "\n",
    "def clean_tweets(tweets):\n",
    "    \"\"\"Removes whitespace and non-essential characters from tweets\"\"\"\n",
    "    for i in range(len(tweets)):\n",
    "        tweet = tweets[i]\n",
    "        tweet = re.sub(r\"((www.)?https?:\\/\\/)?[^\\s]*\\.([\\w]{2,3})(\\/\\w*)*\", \"\", tweet) #removes links\n",
    "        tweet = re.sub(r\"(RT )?@\\w+:?\", \"\", tweet) #removes RT and @\n",
    "        tweet = re.sub(r\"[^(a-zA-Z|\\')]\", \" \", tweet)\n",
    "        tweet = ' '.join(tweet.split()) #removes whitespace from text\n",
    "        tweets[i] = tweet\n",
    "    final_tweets = list(filter(lambda tweet: tweet != \"\", tweets))\n",
    "    return final_tweets\n",
    "\n",
    "\n",
    "def run(user=\"G2Jankos\"):\n",
    "    \"\"\"Requests from Twitter API\"\"\"\n",
    "    url = create_url(user)\n",
    "    bearer_token = get_token()\n",
    "    headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    d = json.loads(response.text)\n",
    "    try:\n",
    "        tweets = [entry['text'] for entry in d['data']]\n",
    "        cleaned_tweets = clean_tweets(tweets)\n",
    "        return cleaned_tweets\n",
    "    except KeyError as e:\n",
    "        print('Username not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments(user=None):\n",
    "    tweets = run(user)\n",
    "    total_pos = 0\n",
    "    for tweet in tweets[:10]:\n",
    "        token = tweet_pipeline(tweet)\n",
    "        sentiment = classifier.prob_classify(token)\n",
    "        pos = sentiment.prob('Positive')\n",
    "        total_pos += pos\n",
    "        print(f\"Tweet: {tweet}\\nNegative: {sentiment.prob('Negative')}\\nPositive:{pos}\\n\")\n",
    "    print(total_pos / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Ayyyyyeeeee That s cause is a BEAST Congrats QB\n",
      "Negative: 0.3678654890484291\n",
      "Positive:0.6321345109515714\n",
      "\n",
      "Tweet: FACTS FACTS FACTS It literally makes ABSOLUTELY ZERO SENSE They say if common sense was common then we d all have it Ain t that the truth FreeKyrie\n",
      "Negative: 0.2822793578434904\n",
      "Positive:0.7177206421565137\n",
      "\n",
      "Tweet: Coach POP CONGRATULATIONS ALL TIME WINS\n",
      "Negative: 0.05164349473645165\n",
      "Positive:0.9483565052635501\n",
      "\n",
      "Tweet: I love that the refs let KD and JE talk that talk to each other and didn t TECH them up That s DOPE Understanding the assignment\n",
      "Negative: 0.18738887806514876\n",
      "Positive:0.8126111219348497\n",
      "\n",
      "0.3110822780306485\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
